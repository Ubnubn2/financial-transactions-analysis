# Задание 2: Краулер документов с полнотекстовым поиском

## Описание

Проект представляет собой систему для сканирования хранилища офисных документов (txt, docx, xlsx, pdf), включая вложенные архивы (zip, rar, 7z), извлечения текстового содержимого и загрузки полученных данных в базу PostgreSQL с поддержкой полнотекстового поиска.

## Структура проекта

```
task2_crawler/
├── scripts/
│   ├── generate_files.py   # Генерация тестовых файлов разных форматов
│   ├── crawler.py           # Сканирование хранилища, парсинг файлов, создание CSV
│   └── load_to_db.py        # Загрузка CSV в PostgreSQL и создание полнотекстового индекса
├── Dockerfile               # Образ для запуска Python-скриптов
├── docker-compose.yml       # Оркестрация PostgreSQL и приложения
├── requirements.txt         # Зависимости Python
├── README.md                # Этот файл
└── (после запуска появятся папки test_storage/ и data/)
```

## Требования

- Установленный **Docker** и **Docker Compose** (для Windows рекомендуется Docker Desktop).
- Для работы с RAR-архивами необходима утилита **unrar**, доступная в системе:
  - Windows: установите [WinRAR](https://www.win-rar.com/) и добавьте путь к `unrar.exe` в переменную PATH.
  - Linux: `sudo apt install unrar`
  - macOS: `brew install unrar`
- (Опционально) Python 3.8+ для запуска скриптов вне Docker, но рекомендуется использовать контейнеры.

## Быстрый старт

### 1. Клонируйте репозиторий и перейдите в папку задания
```bash
cd task2_crawler
```bash

## 2. Запустите контейнеры
```
docker-compose up -d
```
Будут запущены:
- **postgres-fts** – контейнер с PostgreSQL (порт 5432, база `search_db`, пользователь `postgres`, пароль `mysecretpassword`).
- **crawler-app** – контейнер с Python-скриптами (остановится сразу, так как не содержит долгого процесса; мы будем запускать скрипты через `docker-compose run`).

### 3. Сгенерируйте тестовые файлы
```bash
docker-compose run --rm app python scripts/generate_files.py
```
После выполнения в текущей папке появится директория `test_storage/` с набором файлов: .txt, .docx, .xlsx, .pdf, а также архивы .zip, .rar, .7z.

### 4. Запустите краулер
```bash
docker-compose run --rm app python scripts/crawler.py
```
Скрипт рекурсивно обойдёт `test_storage`, распакует архивы во временные папки, извлечёт текст из всех поддерживаемых форматов и сохранит результаты в `data/crawled_data.csv`.

### 5. Загрузите данные в PostgreSQL
```bash
docker-compose run --rm app python scripts/load_to_db.py
```
Скрипт:
- Подключается к PostgreSQL (параметры берутся из переменных окружения, заданных в `docker-compose.yml`).
- Создаёт таблицу `files` и заполняет её данными из CSV.
- Добавляет колонку `content_tsv` типа `tsvector` и строит GIN-индекс для полнотекстового поиска.
- Выполняет тестовый поиск по слову `John` (при необходимости измените запрос в коде).

### 6. Выполните поиск вручную (опционально)
Подключитесь к базе и выполните произвольный запрос:
```bash
docker exec -it postgres-fts psql -U postgres -d search_db
```
Пример поиска с выделением фрагментов:
```sql
SELECT file_path,
       ts_headline('russian', content, plainto_tsquery('russian', 'John')) as snippet
FROM files
WHERE content_tsv @@ plainto_tsquery('russian', 'John')
ORDER BY ts_rank(content_tsv, plainto_tsquery('russian', 'John')) DESC;
```

## Как это работает

### Генерация файлов (`generate_files.py`)
Использует библиотеку `Faker` для создания текстового наполнения на английском языке. Создаются:
- Текстовые файлы (.txt)
- Документы Word (.docx)
- Электронные таблицы (.xlsx)
- PDF-документы (.pdf)
- Архивы .zip, .rar, .7z, содержащие внутри те же типы файлов.

### Краулер (`crawler.py`)
- Рекурсивно обходит все подпапки `test_storage`.
- Для каждого найденного файла определяет расширение и вызывает соответствующую функцию извлечения текста:
  - `.txt` – чтение UTF-8.
  - `.docx` – парсинг через `python-docx`.
  - `.xlsx` – чтение через `openpyxl`.
  - `.pdf` – извлечение текста через `pdfplumber`.
  - Архивы распаковываются во временную директорию, после чего извлечённые файлы обрабатываются рекурсивно.
- Результат сохраняется в `data/crawled_data.csv` с колонками: `file_path`, `file_name`, `file_type`, `content`.

### Загрузка в PostgreSQL (`load_to_db.py`)
- Читает CSV и вставляет записи в таблицу `files`.
- Создаёт колонку `content_tsv` на основе содержимого с учётом русского и английского языков.
- Строит GIN-индекс для быстрого поиска.
- Демонстрирует пример поиска с ранжированием результатов.

## Запуск без Docker (для разработки)

Если вы предпочитаете запускать скрипты напрямую в Python-окружении:

1. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```
2. Убедитесь, что утилита `unrar` доступна в системе.
3. Запустите скрипты по порядку:
   ```bash
   python scripts/generate_files.py
   python scripts/crawler.py
   python scripts/load_to_db.py
   ```
4. Для подключения к PostgreSQL измените параметры в `load_to_db.py` (по умолчанию они берутся из переменных окружения или используются `localhost`).

## Примечания

- Поддержка `.rar` требует наличия внешней утилиты `unrar`. Если она не установлена, при обработке RAR-архивов будут возникать ошибки – можно временно исключить их из тестовых данных или удалить соответствующие строки в скриптах.
- Для формата `.doc` (старый Word) поддержка не реализована, но при необходимости можно добавить библиотеку `catdoc` или `antiword`.
- Все сгенерированные и промежуточные данные (`test_storage/`, `data/`) добавлены в `.gitignore`, чтобы не засорять репозиторий.

## Возможные улучшения

- Добавить поддержку `.doc` и других форматов.
- Реализовать более сложный поиск с использованием весов (например, учитывать заголовки отдельно).
- Добавить веб-интерфейс для поиска.

## Лицензия

Этот проект выполнен в рамках тестового задания и не предназначен для коммерческого использования.
```
   
