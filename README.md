# Тестовые задания

Репозиторий содержит решения двух тестовых заданий:

1. **Анализ финансовых транзакций и клиентов** (папка `task1_analysis/`).
2. **Разработка поисковика для документов** (папка `task2_crawler/`).

## Общее описание

Каждое задание выполнено в соответствии с требованиями, код снабжён комментариями, результаты представлены в виде ноутбука и рабочих скриптов с Docker-контейнеризацией.

## Содержание

- `task1_analysis/` – анализ данных с помощью pandas, визуализация, прогнозирование.
- `task2_crawler/` – краулер файлов, извлечение текста, загрузка в PostgreSQL с полнотекстовым поиском.

Подробности – в README каждой подпапки.



# Задание 1: Анализ финансовых транзакций и клиентов

## Назначение
Проект выполняет очистку и анализ данных о финансовых транзакциях, объединяет их с данными клиентов и проводит сегментацию по уровню активов.

## Структура проекта
- `src/main.ipynb` - Jupyter Notebook с полным анализом
- `src/crawler.py` - скрипт для краулера (дополнительное задание)
- `tests/test_main.py` - тесты для проверки функций
- `docs/design.md` - описание архитектурных решений
- `transactions_cleaned.csv` - очищенные данные (создается в процессе)

## Запуск проекта
1. Установите зависимости:
   ```bash
   pip install pandas numpy openpyxl matplotlib seaborn scikit-learn


## Задание 2: Краулер документов с полнотекстовым поиском

Вторая часть проекта — разработка системы для сканирования хранилища файлов (txt, docx, xlsx, pdf, а также архивов zip, rar, 7z), извлечения текста и загрузки в PostgreSQL с полнотекстовым поиском.

### Структура папки `task2_crawler/`

- `scripts/` – скрипты на Python:
  - `generate_files.py` – генерация тестовых файлов.
  - `crawler.py` – краулинг и извлечение текста в CSV.
  - `load_to_db.py` – загрузка CSV в PostgreSQL и создание индекса.
- `Dockerfile` – образ для запуска скриптов.
- `docker-compose.yml` – оркестрация PostgreSQL и приложения.
- `requirements.txt` – зависимости Python для этого задания.

### Как запустить

1. Убедитесь, что Docker и Docker Compose установлены.
2. Перейдите в папку задания:
   ```bash
   cd task2_crawler